{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM6f2sJhDeW/rsfznbOBAIz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-I3303AgCjr3","executionInfo":{"status":"ok","timestamp":1651152208510,"user_tz":-480,"elapsed":3558,"user":{"displayName":"Zi Shan","userId":"08195523696677153272"}},"outputId":"17b5d6c1-6fd2-46eb-9851-49f3ce2e80bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["pip install HTMLParser"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CtdPDlrgv2hk","executionInfo":{"status":"ok","timestamp":1651152212374,"user_tz":-480,"elapsed":3870,"user":{"displayName":"Zi Shan","userId":"08195523696677153272"}},"outputId":"68c7326b-bca4-4a6e-fc41-e0cec028698a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: HTMLParser in /usr/local/lib/python3.7/dist-packages (0.0.2)\n"]}]},{"cell_type":"code","source":["pip install transformers==3.0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bz_w169jv-Yq","executionInfo":{"status":"ok","timestamp":1651152216406,"user_tz":-480,"elapsed":4042,"user":{"displayName":"Zi Shan","userId":"08195523696677153272"}},"outputId":"33ae22f0-667c-48f6-bf3c-5f0b894f3b70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.64.0)\n","Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (21.3)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.1.96)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.0.49)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (3.0.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n"]}]},{"cell_type":"code","source":["import json\n","import tqdm\n","\n","path = \"/content/drive/My Drive/5001_Group/\"\n","js = open(path + \"Sarcasm_Headlines_Dataset.json\")\n","dataset = []\n","for line in js.readlines():\n","  js_line = json.loads(line)\n","  dataset.append(js_line)\n","\n","headlines = []\n","labels = []\n","for item in dataset:\n","  headlines.append(item[\"headline\"])\n","  labels.append(int(item[\"is_sarcastic\"]))\n","\n","path = \"/content/drive/My Drive/5001_Group/\"\n","js = open(path + \"Sarcasm_Headlines_Dataset.json\")\n","dataset = []\n","for line in js.readlines():\n","  js_line = json.loads(line)\n","  dataset.append(js_line)\n","\n","articals = []\n","with open(\"/content/drive/My Drive/5001_Group/articals.txt\", \"r\") as f:\n","  lines = f.readlines()\n","f.close()\n","for line in lines:\n","  articals.append(line)"],"metadata":{"id":"UquJKTQgwCMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","import numpy as np\n","from transformers import BertTokenizer\n","\n","class BERTDataset(Dataset):\n","  def __init__(self, sent, labels, max_seq_len):\n","    self.sent = sent\n","    self.labels = labels\n","    self.max_seq_len = max_seq_len\n","    self.dataset = self.preprocess(sent, labels)\n","\n","  def preprocess(self, sent, labels):\n","    data = []\n","    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n","    for i in range(len(labels)):\n","      ids = bert_tokenizer.__call__(sent[i], max_length=self.max_seq_len, padding='max_length', pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n","      data.append([ids[\"input_ids\"], ids[\"attention_mask\"], labels[i]])\n","    return data\n","\n","  def __getitem__(self, idx):\n","    ids = self.dataset[idx][0]\n","    mask = self.dataset[idx][1]\n","    label = self.dataset[idx][2]\n","    item = {}\n","    item['input_ids'] = ids\n","    item['mask'] = mask\n","    item[\"label\"] = label\n","    return item\n","\n","  def __len__(self):\n","    return len(self.dataset)\n","  \n","  def collate_fn(self, batch):\n","    input_ids = [x[\"input_ids\"] for x in batch]\n","    masks = [x[\"mask\"] for x in batch]\n","    labels = [x[\"label\"] for x in batch]\n","\n","    batch_len = len(input_ids)\n","\n","    batch_labels = np.ones((batch_len))\n","    for i in range(batch_len):\n","      batch_labels[i] = labels[i]\n","\n","    batch_ids = torch.tensor(input_ids, dtype=torch.long)\n","    batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n","    batch_masks = torch.tensor(masks, dtype=torch.bool)\n","\n","    return [batch_ids, batch_labels, batch_masks]"],"metadata":{"id":"GQSkmSvvwFRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers.modeling_bert import *\n","from torch.nn.utils.rnn import pad_sequence\n","\n","class Bert(BertPreTrainedModel):\n","  def __init__(self,config):\n","    super(Bert, self).__init__(config)\n","    self.num_labels = 2\n","    self.bert = BertModel(config)\n","    self.dropout = nn.Dropout(0.1)\n","    self.classifier = nn.Linear(768, self.num_labels)\n","    self.init_weights()\n","\n","  def forward(self, input_ids, token_type_ids=None, mask=None, labels=None, batch_len = None):\n","    _, output = self.bert(input_ids=input_ids,\n","                         attention_mask=mask,\n","                         token_type_ids=token_type_ids,\n","                         output_hidden_states=False)\n","    dropout_output = self.dropout(output)\n","    logits = self.classifier(dropout_output)\n","    outputs = (logits,)\n","\n","    if labels is not None:\n","      loss_func = nn.CrossEntropyLoss()\n","      loss = loss_func(logits.view(-1, 2), labels.view(-1))\n","      outputs = (loss,) + outputs\n","\n","    return outputs"],"metadata":{"id":"il_eaK27yMN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import logging\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","def train_epoch(train_loader, model, optimizer, scheduler, epoch):\n","  model.train()\n","  train_losses = 0\n","  for idx, batch_samples in enumerate(tqdm(train_loader)):\n","    batch_ids, batch_labels, batch_masks = batch_samples\n","\n","    batch_ids = batch_ids.to(device)\n","    batch_labels = batch_labels.to(device)\n","    batch_masks = batch_masks.to(device)\n","\n","    loss = model(batch_ids, labels=batch_labels, mask = batch_masks)\n","    loss = loss[0]\n","    train_losses = train_losses + loss.item()\n","    model.zero_grad()\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5)\n","    optimizer.step()\n","    scheduler.step()\n","\n","  train_loss = float(train_losses) / len(train_loader)\n","  print(\"Epoch: {}, train loss: {}\".format(epoch, train_loss))\n","\n","def dev_epoch(dev_loader, model, optimizer, scheduler, epoch):\n","  model.eval()\n","  dev_losses = 0\n","  pred_labels = []\n","  true_labels = []\n","  for idx, batch_samples in enumerate(tqdm(dev_loader)):\n","    batch_ids, batch_labels, batch_masks = batch_samples\n","\n","    batch_ids = batch_ids.to(device)\n","    batch_labels = batch_labels.to(device)\n","    batch_masks = batch_masks.to(device)\n","\n","    batch_output = model(batch_ids, mask = batch_masks)\n","    batch_output= batch_output[0].detach().cpu().numpy()\n","    \n","    batch_labels = batch_labels.to('cpu').numpy()\n","    pred_labels.extend(np.argmax(batch_output, axis=1))\n","    true_labels.extend(batch_labels)\n","\n","  f1 = f1_score(pred_labels, true_labels)\n","  print(\"Epoch: {}, f1: {}\".format(epoch, f1 * 100))\n","\n","def train_model(train_loader, dev_loader, model, optimizer, scheduler, epoch_num):\n","  for epoch in range(1, epoch_num + 1):\n","    train_epoch(train_loader, model, optimizer, scheduler, epoch)\n","    dev_epoch(dev_loader, model, optimizer, scheduler, epoch)\n","  print(\"Training Finished!\")\n","\n","def test_model(test_loader, model, optimizer, scheduler):\n","  model.eval()\n","  pred_labels = []\n","  true_labels = []\n","  for idx, batch_samples in enumerate(tqdm(test_loader)):\n","    batch_ids, batch_labels, batch_masks = batch_samples\n","\n","    batch_ids = batch_ids.to(device)\n","    batch_labels = batch_labels.to(device)\n","    batch_masks = batch_masks.to(device)\n","\n","    batch_output = model(batch_ids, mask=batch_masks)\n","    batch_output= batch_output[0].detach().cpu().numpy()\n","    batch_labels = batch_labels.to('cpu').numpy()\n","    pred_labels.extend(np.argmax(batch_output, axis=1))\n","    true_labels.extend(batch_labels)\n","\n","  f1 = f1_score(pred_labels, true_labels)\n","  print(\"test f1: {}\".format(f1 * 100))\n","  return (pred_labels, true_labels)"],"metadata":{"id":"k8gJiHXmzHb3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_rest, y_train, y_rest = train_test_split(articals, labels, test_size = 0.3, random_state=1)\n","X_dev, X_test, y_dev, y_test = train_test_split(X_rest, y_rest, test_size = 2/3, random_state=1)\n","\n","max_seq_length = 128\n","training_set = BERTDataset(X_train, y_train, max_seq_length)\n","dev_set = BERTDataset(X_dev, y_dev, max_seq_length)\n","test_set = BERTDataset(X_test, y_test, max_seq_length)"],"metadata":{"id":"9wrSkeUdzx6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers.optimization import get_cosine_schedule_with_warmup, AdamW\n","\n","train_loader = DataLoader(training_set, batch_size=64, shuffle=False, collate_fn=training_set.collate_fn)\n","dev_loader = DataLoader(dev_set, batch_size=64, shuffle=False, collate_fn=dev_set.collate_fn)\n","test_loader = DataLoader(test_set, batch_size=64, shuffle=False, collate_fn=test_set.collate_fn)\n","\n","model = Bert.from_pretrained(\"bert-base-uncased\")\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","model.to(device)\n","bert_optimizer = list(model.bert.named_parameters())\n","classifier_optimizer = list(model.classifier.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in bert_optimizer if not any(nd in n for nd in no_decay)],\n","      'lr': 1e-5, 'weight_decay': 0.01},\n","    {'params': [p for n, p in bert_optimizer if any(nd in n for nd in no_decay)],\n","      'lr': 1e-5, 'weight_decay': 0.0},\n","    {'params': [p for n, p in classifier_optimizer if not any(nd in n for nd in no_decay)],\n","      'lr': 5e-4, 'weight_decay': 0.01},\n","    {'params': [p for n, p in classifier_optimizer if any(nd in n for nd in no_decay)],\n","      'lr': 5e-4, 'weight_decay': 0.0}]\n","\n","epoch_num = 3\n","optimizer = AdamW(optimizer_grouped_parameters, lr=5e-4, correct_bias=False)\n","train_size = len(training_set)\n","train_steps_per_epoch = train_size // 64\n","scheduler = get_cosine_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps=train_steps_per_epoch,\n","                                            num_training_steps=epoch_num * train_steps_per_epoch)\n","print(\"--------Start Training!--------\")\n","train_model(train_loader, dev_loader, model, optimizer, scheduler, epoch_num)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLweCQx10Gz6","executionInfo":{"status":"ok","timestamp":1651153649866,"user_tz":-480,"elapsed":1290446,"user":{"displayName":"Zi Shan","userId":"08195523696677153272"}},"outputId":"e5c5cfd4-da01-43b5-eca3-1ad6e5d42dd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing Bert: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing Bert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing Bert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of Bert were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["cuda\n","--------Start Training!--------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 293/293 [06:38<00:00,  1.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1, train loss: 0.3679056069098489\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 42/42 [00:21<00:00,  1.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1, f1: 91.52139461172742\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 293/293 [06:49<00:00,  1.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2, train loss: 0.1537201500596294\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 42/42 [00:21<00:00,  1.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2, f1: 93.81907490790012\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 293/293 [06:49<00:00,  1.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3, train loss: 0.11761340398640203\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 42/42 [00:21<00:00,  1.98it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3, f1: 94.09351927809679\n","Training Finished!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["pred_labels, true_labels = test_model(test_loader, model, optimizer, scheduler)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JNQc20lA5bn-","executionInfo":{"status":"ok","timestamp":1651153692081,"user_tz":-480,"elapsed":42232,"user":{"displayName":"Zi Shan","userId":"08195523696677153272"}},"outputId":"28bef735-5d0b-4a76-8c52-cb0290ce491b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:42<00:00,  1.98it/s]"]},{"output_type":"stream","name":"stdout","text":["test f1: 94.33042064621013\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"f3sTRopP5WXV"},"execution_count":null,"outputs":[]}]}